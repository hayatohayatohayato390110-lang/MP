import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import rasterio
from rasterio.windows import Window
from rasterio.warp import transform
import geopandas as gpd
import pandas as pd
import numpy as np
import glob
import random
import time
import zipfile
import gc
from shapely.geometry import Point
import matplotlib.pyplot as plt

# 機械学習
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input

# =========================================================================
# 【設定】 最終版 (Auto-Center from Teacher Data)
# =========================================================================
SHP_PATH = r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\multiforestbunnruidata.scpx"
MAIN_IMAGE_DIR = r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20250202"
SAFE_DIRS = [
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20241114",
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20241219",
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20250118",
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20250202",
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20250319"
]
OUTPUT_DIR = r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2"
OUTPUT_MAP = os.path.join(OUTPUT_DIR, "Final_Correct_Location_Map.tif")

# 解析半径 (教師データの中心から)
RADIUS_KM = 30 

# パラメータ
PATCH_SIZE = 32
UPSCALE_SIZE = 128
POINTS_PER_POLYGON = 50
GRID_STRIDE = 1 
BLOCK_SIZE = 512
BATCH_SIZE = 64
# =========================================================================

def find_band(safe_dir, band):
    pattern = os.path.join(safe_dir, "**", f"*{band}*.jp2")
    files = glob.glob(pattern, recursive=True)
    target = [f for f in files if "TCI" not in f and "PVI" not in f and ("_B" in f)]
    return target[0] if target else None

print("=== 最終版解析開始 (位置自動補正 & B12完全対応) ===")

# 1. バンドパス特定
target_bands = ["B02", "B03", "B04", "B08", "B11", "B12"]
print("1. 画像パスを特定中...")
main_paths = {b: find_band(MAIN_IMAGE_DIR, b) for b in target_bands}
ts_paths = []
for d in SAFE_DIRS:
    found = {b: find_band(d, b) for b in target_bands}
    if all(found.values()): ts_paths.append(found)

if not all(main_paths.values()):
    raise FileNotFoundError("必須バンドが見つかりません。")

# 画像情報の取得 (B08基準)
with rasterio.open(main_paths["B08"]) as src:
    profile = src.profile.copy()
    src_transform = src.transform
    src_crs = src.crs
    src_height, src_width = src.height, src.width
    src_bounds = src.bounds

# --- 2. 教師データの読み込み & 中心位置の特定 ---
print("2. 教師データから解析エリアを自動計算中...")
extract_dir = os.path.dirname(SHP_PATH)
gpkg_name = "geometry.gpkg"
gpkg_path = os.path.join(extract_dir, gpkg_name)

try:
    with zipfile.ZipFile(SHP_PATH, 'r') as z:
        if gpkg_name in z.namelist(): z.extract(gpkg_name, extract_dir)
    gdf = gpd.read_file(gpkg_path)
except: gdf = gpd.read_file(gpkg_path)

# 教師データを画像と同じ座標系に変換
if gdf.crs != src_crs:
    gdf = gdf.to_crs(src_crs)

# ★ここが修正ポイント：教師データの「重心」を計算
total_bounds = gdf.total_bounds # (minx, miny, maxx, maxy)
center_x = (total_bounds[0] + total_bounds[2]) / 2
center_y = (total_bounds[1] + total_bounds[3]) / 2
print(f"   -> 教師データの中心: {center_x:.1f}, {center_y:.1f} (UTM座標)")

# 中心から半径30kmの範囲を計算
radius_m = RADIUS_KM * 1000.0
roi_min_x = max(src_bounds.left, center_x - radius_m)
roi_max_x = min(src_bounds.right, center_x + radius_m)
roi_min_y = max(src_bounds.bottom, center_y - radius_m)
roi_max_y = min(src_bounds.top, center_y + radius_m)

# ピクセル座標に変換
window_full = rasterio.windows.from_bounds(roi_min_x, roi_min_y, roi_max_x, roi_max_y, transform=src_transform)
roi_window = window_full.round_offsets().round_shape()

roi_height = int(roi_window.height)
roi_width = int(roi_window.width)
roi_transform = rasterio.windows.transform(roi_window, src_transform)

print(f"   -> 確定した切り抜きサイズ: {roi_width}x{roi_height} ピクセル")
profile.update(height=roi_height, width=roi_width, transform=roi_transform, count=1, dtype=rasterio.uint8)

# --- 指数計算 (B12対応) ---
def calc_indices(b2, b3, b4, b8, b11, b12):
    eps = 1e-6
    # NDVI (植生)
    ndvi = (b8 - b4) / (b8 + b4 + eps)
    # NDWI (水)
    ndwi = (b3 - b8) / (b3 + b8 + eps)
    # NDBI (都市)
    ndbi = (b11 - b8) / (b11 + b8 + eps)
    # EVI (拡張植生)
    s = 10000.0
    evi = 2.5 * ((b8/s - b4/s) / (b8/s + 6*(b4/s) - 7.5*(b2/s) + 1 + eps))
    # BSI (裸地)
    bsi = ((b11 + b4) - (b8 + b2)) / ((b11 + b4) + (b8 + b2) + eps)
    return [ndvi, ndwi, ndbi, evi, bsi]

# --- 3. データ取得関数 (地理座標対応) ---
def get_point_data(p_geom):
    try:
        # A. 画像 (Inception用)
        py, px = ~src_transform * (p_geom.x, p_geom.y)
        px, py = int(px), int(py)
        
        half = PATCH_SIZE // 2
        # 画像範囲外チェック
        if py < half or py >= src_height-half or px < half or px >= src_width-half: return None
        
        window = Window(px - half, py - half, PATCH_SIZE, PATCH_SIZE)
        with rasterio.open(main_paths["B08"]) as s8, \
             rasterio.open(main_paths["B04"]) as s4, \
             rasterio.open(main_paths["B03"]) as s3:
            p8 = s8.read(1, window=window).astype('float32')
            p4 = s4.read(1, window=window).astype('float32')
            p3 = s3.read(1, window=window).astype('float32')
        
        if p8.shape != (PATCH_SIZE, PATCH_SIZE): return None
        img = np.dstack([p8, p4, p3])
        denom = np.max(img) - np.min(img)
        img = (img - np.min(img)) / (denom + 1e-6) * 255.0
        img_resized = tf.image.resize(img, (UPSCALE_SIZE, UPSCALE_SIZE))
        input_img = preprocess_input(img_resized)

        # B. 時系列指数
        ts_features = []
        for paths in ts_paths:
            vals = {}
            for b_name in target_bands:
                with rasterio.open(paths[b_name]) as src:
                    # 地理座標から取得 (解像度ズレ吸収)
                    r, c = src.index(p_geom.x, p_geom.y)
                    if 0 <= r < src.height and 0 <= c < src.width:
                         vals[b_name] = src.read(1, window=Window(c, r, 1, 1))[0, 0].astype('float32')
                    else: vals[b_name] = 0.0
            
            if all(v > 0 for v in vals.values()):
                indices = calc_indices(vals["B02"], vals["B03"], vals["B04"], vals["B08"], vals["B11"], vals["B12"])
                ts_features.append(vals["B08"])
                ts_features.append(vals["B12"])
                ts_features.extend(indices)
            else:
                ts_features.extend([0.0] * 7)

        return input_img, np.array(ts_features)
    except: return None

# --- 4. Inception準備 ---
print("3. AIモデル準備...")
inception = InceptionV3(weights='imagenet', include_top=False, 
                        input_shape=(UPSCALE_SIZE, UPSCALE_SIZE, 3), pooling='avg')
inception.trainable = False

# --- 5. 教師データ収集 (再) ---
print("4. 教師データを生成中...")
# クラス列特定
target_col = None
for col in ['class_id', 'macroclass_id', 'C_ID', 'MC_ID']:
    if col in gdf.columns: target_col = col; break
if target_col is None: target_col = gdf.columns[0]

X_img_buffer = []
X_ts_buffer = []
y_labels = []

print("   サンプリング開始...")
for idx, row in gdf.iterrows():
    label = int(row[target_col])
    geom = row.geometry
    points = []
    if geom.geom_type in ['Polygon', 'MultiPolygon']:
        minx, miny, maxx, maxy = geom.bounds
        attempts = 0
        while len(points) < POINTS_PER_POLYGON and attempts < POINTS_PER_POLYGON*5:
            pnt = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))
            if geom.contains(pnt): points.append(pnt)
            attempts += 1
    else: points.append(geom)

    for p in points:
        res = get_point_data(p)
        if res is None: continue
        img, ts = res
        X_img_buffer.append(img)
        X_ts_buffer.append(ts)
        y_labels.append(label)
    
    if len(y_labels) > 0 and len(y_labels) % 500 == 0: 
        print(f"   ...{len(y_labels)}点", end="\r")

X_imgs = np.array(X_img_buffer)
X_ts = np.array(X_ts_buffer)
y = np.array(y_labels)

# --- 6. SVM学習 ---
print(f"\n5. SVM学習中 (データ数: {len(y)})...")
X_img_features = inception.predict(X_imgs, batch_size=32, verbose=0)
X_combined = np.concatenate([X_img_features, X_ts], axis=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_combined)

le = LabelEncoder()
y_encoded = le.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

svm = SVC(kernel='rbf', C=1.0, probability=False, random_state=42)
svm.fit(X_train, y_train)
print(f"   Train Acc: {svm.score(X_train, y_train)*100:.2f}%, Test Acc: {svm.score(X_test, y_test)*100:.2f}%")

# --- 7. マップ作成 (自動補正エリア) ---
print("\n6. マップ作成 (教師データ周辺エリア)...")
final_map = np.zeros((roi_height, roi_width), dtype=np.uint8)
half = PATCH_SIZE // 2

# ファイルオープン
srcs = {} 
for t_idx, paths in enumerate([main_paths] + ts_paths): 
    key = 'main' if t_idx == 0 else t_idx - 1 
    srcs[key] = {}
    for b_name in target_bands:
        srcs[key][b_name] = rasterio.open(paths[b_name])

start_time = time.time()

try:
    for r_blk in range(0, roi_height, BLOCK_SIZE):
        for c_blk in range(0, roi_width, BLOCK_SIZE):
            
            # ROIウィンドウ上のブロック範囲
            abs_r_start = roi_window.row_off + r_blk - half
            abs_c_start = roi_window.col_off + c_blk - half
            
            # ブロックサイズ (ROI境界考慮)
            r_end_blk = min(roi_height, r_blk + BLOCK_SIZE)
            c_end_blk = min(roi_width, c_blk + BLOCK_SIZE)
            
            abs_h = (r_end_blk - r_blk) + 2*half
            abs_w = (c_end_blk - c_blk) + 2*half
            
            # 地理座標バウンディングボックスを取得
            blk_win = Window(abs_c_start, abs_r_start, abs_w, abs_h)
            blk_bounds = rasterio.windows.bounds(blk_win, src_transform)
            
            # --- データロード (Boundsベース) ---
            loaded_data = {} 
            skip_block = False
            
            try:
                for key, band_srcs in srcs.items():
                    loaded_data[key] = {}
                    for b_name, src in band_srcs.items():
                        # 地理的範囲からWindowを逆算 (これで解像度ズレ解消)
                        src_win = src.window(*blk_bounds)
                        # 強制リサイズ読み込み
                        data = src.read(1, window=src_win, out_shape=(int(abs_h), int(abs_w)), resampling=5)
                        loaded_data[key][b_name] = data.astype('float32')
            except Exception: skip_block = True
            
            if skip_block: continue

            # 推論ループ
            batch_imgs = []
            batch_ts = []
            coords = []
            
            valid_h = r_end_blk - r_blk
            valid_w = c_end_blk - c_blk
            
            for br in range(0, valid_h, GRID_STRIDE):
                for bc in range(0, valid_w, GRID_STRIDE):
                    lr = br + half
                    lc = bc + half
                    if lr >= abs_h or lc >= abs_w: continue

                    p8 = loaded_data['main']['B08'][lr-half:lr+half, lc-half:lc+half]
                    if p8.shape != (PATCH_SIZE, PATCH_SIZE): continue
                    p4 = loaded_data['main']['B04'][lr-half:lr+half, lc-half:lc+half]
                    p3 = loaded_data['main']['B03'][lr-half:lr+half, lc-half:lc+half]
                    
                    img = np.dstack([p8, p4, p3])
                    denom = np.max(img) - np.min(img)
                    img = (img - np.min(img)) / (denom + 1e-6) * 255.0
                    
                    ts_vec = []
                    for i in range(len(ts_paths)):
                        d = loaded_data[i]
                        v2, v3, v4, v8, v11, v12 = d['B02'][lr,lc], d['B03'][lr,lc], d['B04'][lr,lc], d['B08'][lr,lc], d['B11'][lr,lc], d['B12'][lr,lc]
                        indices = calc_indices(v2, v3, v4, v8, v11, v12)
                        ts_vec.append(v8)
                        ts_vec.append(v12)
                        ts_vec.extend(indices)
                        
                    batch_imgs.append(img)
                    batch_ts.append(np.array(ts_vec))
                    coords.append((br, bc))
                    
                    if len(batch_imgs) >= BATCH_SIZE:
                        imgs_tensor = tf.image.resize(np.array(batch_imgs), (UPSCALE_SIZE, UPSCALE_SIZE))
                        imgs_pre = preprocess_input(imgs_tensor)
                        feats_img = inception.predict(imgs_pre, verbose=0)
                        
                        feats_comb = np.concatenate([feats_img, np.array(batch_ts)], axis=1)
                        feats_scaled = scaler.transform(feats_comb)
                        preds = svm.predict(feats_scaled)
                        
                        for k, (l_br, l_bc) in enumerate(coords):
                            final_map[r_blk + l_br : r_blk + l_br + GRID_STRIDE, 
                                      c_blk + l_bc : c_blk + l_bc + GRID_STRIDE] = preds[k]
                        
                        batch_imgs, batch_ts, coords = [], [], []

            if len(batch_imgs) > 0:
                imgs_tensor = tf.image.resize(np.array(batch_imgs), (UPSCALE_SIZE, UPSCALE_SIZE))
                imgs_pre = preprocess_input(imgs_tensor)
                feats_img = inception.predict(imgs_pre, verbose=0)
                feats_comb = np.concatenate([feats_img, np.array(batch_ts)], axis=1)
                feats_scaled = scaler.transform(feats_comb)
                preds = svm.predict(feats_scaled)
                
                for k, (l_br, l_bc) in enumerate(coords):
                    final_map[r_blk + l_br : r_blk + l_br + GRID_STRIDE, 
                              c_blk + l_bc : c_blk + l_bc + GRID_STRIDE] = preds[k]

            elapsed = time.time() - start_time
            if (r_blk + c_blk) % (BLOCK_SIZE * 2) == 0:
                print(f"   Block {r_blk},{c_blk} 完了 (経過: {elapsed/60:.1f}分)...", end="\r")
            
            del loaded_data
            gc.collect()

finally:
    for group in srcs.values():
        for s in group.values(): s.close()

print("\nファイル保存中...")
with rasterio.open(OUTPUT_MAP, 'w', **profile) as dst:
    dst.write(final_map, 1)

print(f"=== 全工程完了: {OUTPUT_MAP} ===")
