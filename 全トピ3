import os
# TensorFlowログ抑制
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import rasterio
from rasterio.windows import Window
from rasterio.enums import Resampling
from rasterio.features import shapes
import geopandas as gpd
import pandas as pd
import numpy as np
import glob
import random
import time
import zipfile
import gc
import math
from shapely.geometry import shape, Polygon, Point
from shapely.validation import make_valid

# 画像処理・機械学習
from skimage.segmentation import slic
from skimage.measure import regionprops
from skimage.transform import resize
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input

# =========================================================================
# 【設定】 Explainable Full-Spec Analysis (XAI)
# =========================================================================
# 1. バンド設定
TARGET_BANDS_S2 = ["B02", "B03", "B04", "B05", "B06", "B07", "B08", "B8A", "B11", "B12"]
TARGET_BANDS_S1 = ["VV", "VH"]

# 2. パス設定 (ペアリング済みリスト)
# ※必ず「11月→3月」の順で並べてください
S2_DIRS = [
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20241114", # 11月
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20241219", # 12月
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20250118", # 1月
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20250202", # 2月 (メイン)
    r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\20250319"  # 3月
]

S1_DIRS = [
    r"G:\SATELITEIMAGE\RABI\MP\2025 sentinel1\20241115", # 11月
    r"G:\SATELITEIMAGE\RABI\MP\2025 sentinel1\20241221", # 12月
    r"G:\SATELITEIMAGE\RABI\MP\2025 sentinel1\20250114", # 1月
    r"G:\SATELITEIMAGE\RABI\MP\2025 sentinel1\20250207", # 2月 (メイン)
    r"G:\SATELITEIMAGE\RABI\MP\2025 sentinel1\20250315"  # 3月
]

MAIN_IDX = 3 # メイン時期(2月)
SHP_PATH = r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2\multiforestbunnruidata.scpx"

OUTPUT_DIR = r"G:\SATELITEIMAGE\RABI\MP\2025SENTINEL2"
OUTPUT_MAP = os.path.join(OUTPUT_DIR, "Final_XAI_Map.tif")
OUTPUT_TABLE = os.path.join(OUTPUT_DIR, "Final_XAI_Ledger_Full.csv") # 全データ入り台帳
OUTPUT_SUMMARY = os.path.join(OUTPUT_DIR, "Final_XAI_Class_Definition.csv") # 判断基準レポート

# 3. 解析パラメータ
RADIUS_KM = 2.0
UPSCALE_SIZE = 128
APPROX_N_SEGMENTS = 4000 
SEGMENT_COMPACTNESS = 15.0
MAX_PIXEL_VALUE = 3000.0
BATCH_SIZE_CNN = 32

# ★出力クラス名の定義 (教師データのIDに合わせて書き換えてください)
CLASS_NAMES = {
    1: "Wheat (小麦)", 
    2: "Vegetables (野菜)", 
    3: "Forest (森林)", 
    4: "Grassland (草原)", 
    5: "Soil/Fallow (休耕地)"
}

# =========================================================================

# --- 関数定義 (指標名キー付きで返すように変更) ---

def calc_all_indices_dict(val_dict):
    """全指標を辞書形式で返す(CSV出力用)"""
    eps = 1e-6
    b2 = val_dict.get("B02", 0); b3 = val_dict.get("B03", 0); b4 = val_dict.get("B04", 0)
    b5 = val_dict.get("B05", 0); b6 = val_dict.get("B06", 0); b7 = val_dict.get("B07", 0)
    b8 = val_dict.get("B08", 0); b11 = val_dict.get("B11", 0); b12 = val_dict.get("B12", 0)
    
    # 計算
    vals = {}
    vals['NDVI'] = (b8 - b4) / (b8 + b4 + eps)
    vals['GNDVI'] = (b8 - b3) / (b8 + b3 + eps)
    knr = (b8 - b4) / (b8 + b4 + eps); vals['kNDVI'] = np.tanh(knr ** 2)
    vals['EVI'] = 2.5 * ((b8 - b4) / (b8 + 6*b4 - 7.5*b2 + 10000*eps + 1))
    vals['NDRE1'] = (b8 - b5) / (b8 + b5 + eps)
    vals['NDRE2'] = (b8 - b6) / (b8 + b6 + eps)
    vals['CI_re'] = (b8 / (b5 + eps)) - 1
    vals['MCARI'] = ((b5 - b4) - 0.2 * (b5 - b3)) * (b5 / (b4 + eps))
    vals['MNDWI'] = (b3 - b11) / (b3 + b11 + eps)
    vals['NDWI'] = (b3 - b8) / (b3 + b8 + eps)
    vals['LSWI'] = (b8 - b11) / (b8 + b11 + eps)
    vals['SWI'] = (b5 - b11) / (b5 + b11 + eps)
    vals['NDBI'] = (b11 - b8) / (b11 + b8 + eps)
    vals['BSI'] = ((b11 + b4) - (b8 + b2)) / ((b11 + b4) + (b8 + b2) + eps)
    vals['NDBSI'] = (b11 - b8) / (b11 + b8 + eps)
    
    # 生バンドも記録
    vals['Band_SWIR1'] = b11
    vals['Band_NIR'] = b8
    vals['Band_RedEdge'] = b5
    
    # リスト形式(学習用)と辞書形式(表示用)の両方を返す
    # 学習用リストの順番は固定すること
    feat_list = [
        vals['NDVI'], vals['kNDVI'], vals['GNDVI'], vals['EVI'], 
        vals['NDRE1'], vals['NDRE2'], vals['CI_re'], vals['MCARI'],
        vals['MNDWI'], vals['NDWI'], vals['LSWI'], vals['SWI'],
        vals['NDBI'], vals['BSI'], vals['NDBSI']
    ]
    return feat_list, vals

def calc_bio_structural_hysteresis(time_series_means):
    if not time_series_means: return 0.0, 0.0
    x_vals = []; y_vals = []
    for t_data in time_series_means:
        b8 = t_data.get('B08', 0); b4 = t_data.get('B04', 0)
        ndvi = (b8 - b4) / (b8 + b4 + 1e-6)
        vh = t_data.get('VH', 0)
        x_vals.append(ndvi); y_vals.append(vh)
    area = 0.0; n = len(x_vals)
    if n >= 3:
        for i in range(n):
            j = (i + 1) % n; area += x_vals[i] * y_vals[j]; area -= y_vals[i] * x_vals[j];
        area = abs(area) / 2.0
    lag = np.argmax(y_vals) - np.argmax(x_vals)
    return area, lag

def find_band(search_dir, band_name):
    pattern = os.path.join(search_dir, "**", f"*{band_name}*.tif")
    files = glob.glob(pattern, recursive=True)
    if not files:
        pattern = os.path.join(search_dir, "**", f"*{band_name}*.jp2")
        files = glob.glob(pattern, recursive=True)
    target = [f for f in files if "TCI" not in f and "PVI" not in f]
    target = [f for f in target if f.lower().endswith('.tif') or f.lower().endswith('.jp2')]
    return target[0] if target else None

# =========================================================================

print("=== Explainable AI (XAI) Crop Analysis Started ===")

# --- 1. パス準備 ---
print("1. データリンク収集...")
ts_paths_list = []
for i in range(len(S2_DIRS)):
    period_paths = {}
    for b in TARGET_BANDS_S2:
        p = find_band(S2_DIRS[i], b); 
        if p: period_paths[b] = p
    for b in TARGET_BANDS_S1:
        p = find_band(S1_DIRS[i], b); 
        if p: period_paths[b] = p
    if "B08" in period_paths: ts_paths_list.append(period_paths)

main_paths = ts_paths_list[MAIN_IDX]

# --- 2. 画像読み込み & Auto-Segmentation ---
print("2. 自動区画化 (Plot Generation)...")
with rasterio.open(main_paths["B08"]) as src:
    profile = src.profile; transform = src.transform; crs = src.crs
    extract_dir = os.path.dirname(SHP_PATH)
    try: gdf_raw = gpd.read_file(os.path.join(extract_dir, "geometry.gpkg"))
    except: with zipfile.ZipFile(SHP_PATH, 'r') as z: z.extract("geometry.gpkg", extract_dir); gdf_raw = gpd.read_file(os.path.join(extract_dir, "geometry.gpkg"))
    if gdf_raw.crs != crs: gdf_raw = gdf_raw.to_crs(crs)
    center = gdf_raw.geometry.iloc[0].centroid
    radius_m = RADIUS_KM * 1000; minx, maxx = center.x - radius_m, center.x + radius_m; miny, maxy = center.y - radius_m, center.y + radius_m
    window = rasterio.windows.from_bounds(minx, miny, maxx, maxy, transform)
    win_h, win_w = int(window.height), int(window.width)
    win_transform = rasterio.windows.transform(window, transform)

loaded_data = []; ts_cnn_base_imgs = []
for t_idx, t_paths in enumerate(ts_paths_list):
    t_dict = {}
    for b, p in t_paths.items():
        with rasterio.open(p) as src:
            data = src.read(1, window=window, out_shape=(win_h, win_w), resampling=Resampling.bilinear)
            t_dict[b] = data.astype('float32')
    loaded_data.append(t_dict)
    if "B08" in t_dict and "B04" in t_dict and "B03" in t_dict:
        img_t = np.dstack([t_dict["B08"], t_dict["B04"], t_dict["B03"]])
        img_t_norm = np.clip(img_t / MAX_PIXEL_VALUE, 0, 1) * 255.0
        ts_cnn_base_imgs.append(img_t_norm)
    else: ts_cnn_base_imgs.append(np.zeros((win_h, win_w, 3), dtype=np.float32))

main_stack = loaded_data[MAIN_IDX]
seg_img = np.dstack([main_stack["B08"], main_stack["B04"], main_stack["B03"]])
img_for_slic = np.clip(seg_img / MAX_PIXEL_VALUE, 0, 1)
segments = slic(img_for_slic, n_segments=APPROX_N_SEGMENTS, compactness=SEGMENT_COMPACTNESS, sigma=1.0, start_label=1)
props = regionprops(segments)
print(f"   -> {len(props)} 個の区画を作成。")

# --- 3. 全指標計算 & データ記録 ---
print("3. 全数値データの抽出 & 記録 (Transparency Step)...")

inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(UPSCALE_SIZE, UPSCALE_SIZE, 3), pooling='avg')
inception.trainable = False

segment_features_num = [] 
segment_meta = [] # 台帳用の全データをここに詰め込む
cnn_batches = [[] for _ in range(len(ts_paths_list))]
segment_indices_for_cnn = []

start_time = time.time()

for i, region in enumerate(props):
    label_id = region.label
    coords = region.coords
    r_idx, c_idx = coords[:, 0], coords[:, 1]
    min_r, min_c, max_r, max_c = region.bbox
    
    # A. CNNパッチ抽出
    for t_idx, base_img in enumerate(ts_cnn_base_imgs):
        patch = base_img[min_r:max_r, min_c:max_c]
        if patch.shape[0] < 2 or patch.shape[1] < 2: patch_resized = np.zeros((UPSCALE_SIZE, UPSCALE_SIZE, 3))
        else: patch_resized = resize(patch, (UPSCALE_SIZE, UPSCALE_SIZE), preserve_range=True)
        cnn_batches[t_idx].append(preprocess_input(patch_resized))
    segment_indices_for_cnn.append(i)
    
    # B. 数値データ抽出 & 詳細記録
    ts_means = []
    feat_vec = []
    
    # CSV用のメタデータ辞書
    pixel_count = region.area
    area_ha = (pixel_count * (10 * 10)) / 10000.0
    meta_row = {
        'Plot_ID': f"P-{label_id:04d}", 
        'Area_ha': round(area_ha, 3),
        'id': label_id,
        'idx': i
    }
    
    for t_idx, t_data in enumerate(loaded_data):
        t_mean = {}
        for b in (TARGET_BANDS_S2 + TARGET_BANDS_S1):
            if b in t_data: t_mean[b] = np.mean(t_data[b][r_idx, c_idx])
            else: t_mean[b] = 0.0
        ts_means.append(t_mean)
        
        # 指標計算 (辞書で取得してCSVにも記録)
        indices_list, indices_dict = calc_all_indices_dict(t_mean)
        
        # 生データ + 指標を特徴ベクトルへ
        feat_vec.extend([t_mean.get("B08",0), t_mean.get("B11",0), t_mean.get("VH",0)])
        feat_vec.extend(indices_list)
        
        # ★ここがポイント: CSV用にすべての値を記録 (例: "Nov_NDVI", "Jan_VH"...)
        month_prefix = f"T{t_idx+1}" # T1(11月)...T5(3月)
        for k, v in indices_dict.items():
            meta_row[f"{month_prefix}_{k}"] = round(float(v), 4)
        meta_row[f"{month_prefix}_VH"] = round(float(t_mean.get("VH", 0)), 4)
        meta_row[f"{month_prefix}_VV"] = round(float(t_mean.get("VV", 0)), 4)
    
    hyst_area, hyst_lag = calc_bio_structural_hysteresis(ts_means)
    feat_vec.extend([hyst_area, hyst_lag])
    segment_features_num.append(feat_vec)
    
    # ヒステリシス情報もCSVへ
    meta_row['Hysteresis_Area'] = round(hyst_area, 4)
    meta_row['Hysteresis_Lag'] = int(hyst_lag)
    
    segment_meta.append(meta_row)

# CNN推論
print("   CNN特徴抽出中...")
segment_cnn_features = []
temp_cnn_feats = [[] for _ in range(len(ts_paths_list))]
for t_idx, batch_imgs in enumerate(cnn_batches):
    if not batch_imgs: continue
    print(f"   Step {t_idx+1}...", end="\r")
    preds = inception.predict(np.array(batch_imgs), batch_size=BATCH_SIZE_CNN, verbose=0)
    temp_cnn_feats[t_idx] = preds
print("\n   結合中...")
for i in range(len(segment_indices_for_cnn)):
    stacked = np.concatenate([temp_cnn_feats[t][i] for t in range(len(ts_paths_list))])
    segment_cnn_features.append(stacked)

X_all = np.concatenate([np.array(segment_cnn_features), np.array(segment_features_num)], axis=1)

# --- 4. 学習 ---
print("4. 学習 (Teacher Matching)...")
target_col = next((c for c in ['class_id', 'macroclass_id', 'C_ID'] if c in gdf_raw.columns), gdf_raw.columns[0])
X_train, y_train = [], []
train_indices = set()
for idx, row in gdf_raw.iterrows():
    label = int(row[target_col]); t_center = row.geometry.centroid
    py, px = ~win_transform * (t_center.x, t_center.y); py, px = int(py), int(px)
    if 0 <= py < win_h and 0 <= px < win_w:
        seg_id = segments[py, px]
        found = next((item for item in segment_meta if item['id'] == seg_id), None)
        if found and found['idx'] not in train_indices:
            X_train.append(X_all[found['idx']]); y_train.append(label); train_indices.add(found['idx'])

X_train = np.array(X_train); y_train = np.array(y_train)

if len(y_train) > 0:
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    svm = SVC(kernel='rbf', C=10.0, class_weight='balanced', probability=False)
    svm.fit(X_train_scaled, y_train)
    
    # --- 5. 結果出力 & 判断基準レポート作成 ---
    print("5. 結果集計 & レポート作成...")
    X_all_scaled = scaler.transform(X_all)
    preds = svm.predict(X_all_scaled)
    
    csv_rows = []; lut = np.zeros(segments.max() + 1, dtype=np.uint8)
    
    for i, item in enumerate(segment_meta):
        pred_label = preds[i]
        # ★名前で記録
        class_name = CLASS_NAMES.get(pred_label, f"Unknown_{pred_label}")
        
        row = item.copy()
        row['Final_Class'] = class_name # Wheat, Forestなどを書き込む
        del row['idx']; del row['id']
        csv_rows.append(row)
        lut[item['id']] = pred_label

    # A. 全台帳保存
    df_result = pd.DataFrame(csv_rows)
    # カラム順序を整理 (ID, Class, Area, その後に数値データ)
    cols = ['Plot_ID', 'Final_Class', 'Area_ha', 'Hysteresis_Area'] + [c for c in df_result.columns if c not in ['Plot_ID', 'Final_Class', 'Area_ha', 'Hysteresis_Area']]
    df_result = df_result[cols]
    df_result.to_csv(OUTPUT_TABLE, index=False, encoding='utf-8-sig')
    
    # B. 判断基準レポート (Summary) 作成
    print("   ★判断基準(範囲)を逆算中...")
    # クラスごとにグループ化し、各指標の [Min, Mean, Max] を計算
    summary_dfs = []
    
    # 分析対象のカラム（T1_NDVIなどを抽出）
    target_cols = [c for c in df_result.columns if "NDVI" in c or "VH" in c or "SWIR" in c or "Hysteresis" in c]
    
    for cls_name, group in df_result.groupby("Final_Class"):
        desc = group[target_cols].describe().T[['min', 'mean', 'max']]
        desc['Class'] = cls_name
        summary_dfs.append(desc)
    
    if summary_dfs:
        df_summary = pd.concat(summary_dfs)
        df_summary = df_summary.reset_index().rename(columns={'index': 'Indicator'})
        # 見やすい順に並べ替え
        df_summary = df_summary[['Class', 'Indicator', 'min', 'mean', 'max']]
        df_summary.to_csv(OUTPUT_SUMMARY, index=False, encoding='utf-8-sig')

    # C. 地図保存
    final_map = lut[segments]
    profile.update(height=win_h, width=win_w, transform=win_transform, count=1, dtype=rasterio.uint8)
    with rasterio.open(OUTPUT_MAP, 'w', **profile) as dst: dst.write(final_map, 1)
        
    print("=== 全処理完了 ===")
    print(f"詳細台帳CSV: {OUTPUT_TABLE}")
    print(f"判断基準レポート: {OUTPUT_SUMMARY}")
    print(f"分類地図: {OUTPUT_MAP}")
    print("-> レポート(Summary)を見ると、「小麦(Wheat)と判定された区画のNDVIは0.X〜0.Yだった」という事実が確認できます。")

else: print("エラー: 学習データなし")
