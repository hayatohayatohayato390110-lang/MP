import os
import time
import requests
import re
import html
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
from urllib.parse import urlparse, urljoin

# --- å…¨æ–¹ä½URLè¨­å®š ---
START_URLS = [
    "https://www.escortskubota.com/",
    "https://www.escortskubota.com/investors",
    "https://www.escortskubota.com/media-room",
    "https://www.escortskubota.com/careers",
    "https://www.escortskubota.com/sustainability",
    "https://farmtrac.escortskubota.com/products",
    "https://powertrac.escortskubota.com/products",
    "https://exports.escortskubota.com/tractors",
    "https://digitrac.in/tractor",
    "https://agrisolutions.escortskubota.com/",
    "https://agrisolutions.escortskubota.com/rice-transplanter",
    "https://construction-equipment.escortskubota.com/"
]
MAX_PAGES = 3500

class TrilingualTranslator:
    def __init__(self):
        self.ts_ja = GoogleTranslator(source='auto', target='ja')
        self.ts_en = GoogleTranslator(source='auto', target='en')
        self.ts_hi = GoogleTranslator(source='auto', target='hi')
        self.cache = {}
    
    def get_3_versions(self, text):
        if not text: return {"ja": "", "en": "", "hi": ""}
        if text in self.cache: return self.cache[text]
        if re.match(r'^[\d\s\w\-\(\)]+$', text) and len(text) < 10: return {"ja": text, "en": text, "hi": text}
        try:
            res = { 
                "ja": self.ts_ja.translate(text), 
                "en": self.ts_en.translate(text), 
                "hi": self.ts_hi.translate(text) 
            }
            self.cache[text] = res
            return res
        except: return {"ja": text, "en": text, "hi": text}

    # â˜…ã€è¿½åŠ ã€‘æ—¥æœ¬èªã¸ã®å˜ç‹¬ç¿»è¨³ãƒ¡ã‚½ãƒƒãƒ‰
    def to_japanese(self, text):
        if not text: return ""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†
        if text in self.cache: return self.cache[text].get("ja", text)
        try:
            # çŸ­ã™ãã‚‹æ–‡ã‚„è¨˜å·ã ã‘ãªã‚‰ç¿»è¨³ã—ãªã„
            if len(text) < 5 or re.match(r'^[\W\d]+$', text): return text
            return self.ts_ja.translate(text)
        except:
            return text

class ForceMiner:
    def get_any_image(self, soup, base_url):
        if not soup: return ""
        candidates = []
        og = soup.find('meta', property='og:image')
        if og and og.get('content'): return urljoin(base_url, og.get('content'))
        
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy')
            if not src: continue
            src = urljoin(base_url, src)
            if any(x in src.lower() for x in ['.svg', 'logo', 'icon', 'arrow']): continue
            candidates.append(src)
        return candidates[0] if candidates else ""

    def get_spec_data(self, soup):
        if not soup: return ""
        html_out = ""
        tables = soup.find_all('table')
        for t in tables:
            rows = t.find_all('tr')
            if len(rows) > 2 and "menu" not in t.get_text().lower():
                t_str = re.sub(r'class=".*?"', '', str(t))
                t_str = re.sub(r'style=".*?"', '', t_str)
                html_out += t_str
                break
        if not html_out:
            dls = soup.find_all('dl')
            if dls:
                html_out += "<table>"
                for dl in dls[:3]:
                    for dt, dd in zip(dl.find_all('dt'), dl.find_all('dd')):
                        html_out += f"<tr><th>{dt.get_text(strip=True)}</th><td>{dd.get_text(strip=True)}</td></tr>"
                html_out += "</table>"
        if html_out: return f"<div class='extracted-table'>{html_out}</div>"
        return ""

    def get_headers_and_desc(self, soup):
        if not soup: return [], ""
        desc = ""
        meta = soup.find('meta', attrs={'name': 'description'})
        if meta: desc = meta.get('content', '').strip()
        headers = []
        for h in soup.find_all(['h2', 'h3']):
            t = h.get_text(" ", strip=True)
            if 4 < len(t) < 40: headers.append(t)
        return list(set(headers))[:6], desc

    def get_page_summary(self, soup):
        if not soup: return ""
        
        clean_soup = BeautifulSoup(str(soup), 'html.parser')
        for tag in clean_soup(['script', 'style', 'nav', 'footer', 'header', 'noscript', 'iframe']):
            tag.extract()
            
        summary_text = ""
        meta_desc = clean_soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            summary_text = meta_desc.get('content').strip()
        
        if len(summary_text) < 50:
            paragraphs = clean_soup.find_all('p')
            extracted = []
            for p in paragraphs:
                txt = p.get_text(" ", strip=True)
                if len(txt) > 40 and "copyright" not in txt.lower():
                    extracted.append(txt)
                    if len(extracted) >= 2: break
            
            if extracted:
                summary_text = " ".join(extracted)
            else:
                summary_text = clean_soup.get_text(" ", strip=True)

        summary_text = re.sub(r'\s+', ' ', summary_text).strip()
        if len(summary_text) > 200:
            summary_text = summary_text[:200] + "..."
            
        return summary_text

    def get_label_hp(self, soup, url):
        label = ""
        if soup:
            h1 = soup.find('h1')
            if h1: label = h1.get_text(strip=True)
            elif soup.title: label = soup.title.string.split('|')[0].strip()
        if not label: label = urlparse(url).path.split('/')[-1].replace('-',' ').title()
        return label

def crawl_force(start_urls, limit):
    print("=== å…¨æ§‹é€ è§£æï¼‹æ—¥æœ¬èªè¦ç´„ãƒ¢ãƒ¼ãƒ‰ ===")
    print("ãƒšãƒ¼ã‚¸è¦ç´„ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãªãŒã‚‰åé›†ã—ã¦ã„ã¾ã™...")
    
    s = requests.Session()
    s.headers.update({'User-Agent': 'Mozilla/5.0'})
    requests.packages.urllib3.disable_warnings()
    trans = TrilingualTranslator()
    miner = ForceMiner()
    
    cmap = {"ROOT_HUB": []}
    ninfo = {}
    visited = set()
    queue = []
    
    for u in start_urls:
        u = u.rstrip('/')
        if u not in [x for x in cmap["ROOT_HUB"]]:
            cmap["ROOT_HUB"].append(u)
            ninfo[u] = {"labels":{"ja":"ãƒˆãƒƒãƒ—/ã‚«ãƒ†ã‚´ãƒª","en":"Top/Category","hi":"à¤¶à¥€à¤°à¥à¤·"}, "type":"Corporate", "is_prod": False}
        if u not in visited:
            cmap[u] = []
            queue.append((0,u))
            visited.add(u)
            
    cnt = 0
    while queue and cnt < limit:
        queue.sort(key=lambda x: x[0])
        prio, url = queue.pop(0)
        cnt += 1
        if cnt % 20 == 0: print(f"  [{cnt}/{limit}] {url}")
        
        try:
            res = s.get(url, timeout=8)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            raw_lbl = miner.get_label_hp(soup, url)
            labels = trans.get_3_versions(raw_lbl)
            img = miner.get_any_image(soup, url)
            spec_html = miner.get_spec_data(soup)
            headers, desc = miner.get_headers_and_desc(soup)
            
            # è¦ç´„ã‚’å–å¾—ã—ã€æ—¥æœ¬èªã«ç¿»è¨³
            raw_summary = miner.get_page_summary(soup)
            jp_summary = trans.to_japanese(raw_summary) if raw_summary else ""
            
            u_low = url.lower()
            nt = "Page"
            
            if "investors" in u_low: nt = "Investors"
            elif "media" in u_low: nt = "Media"
            elif "career" in u_low: nt = "Careers"
            elif "sustainability" in u_low or "csr" in u_low: nt = "CSR"
            elif "farmtrac" in u_low: nt = "Farmtrac"
            elif "powertrac" in u_low: nt = "Powertrac"
            elif "digitrac" in u_low: nt = "Digitrac"
            elif "construction" in u_low: nt = "Construction"
            elif "agrisolutions" in u_low or "kubota" in u_low: nt = "Kubota"
            elif "about" in u_low: nt = "Corporate"
            
            is_prod = False
            if spec_html: 
                is_prod = True
            elif any(k in u_low for k in ['product','tractor','model','transplanter','harvester','crane','loader']):
                if len(urlparse(url).path) > 2:
                    is_prod = True
                    if nt == "Page": nt = "Product"
            
            ninfo[url] = {
                "labels": labels, "type": nt, "url": url,
                "img": img, "spec": spec_html, "headers": headers, 
                "desc": desc, "page_summary": jp_summary, # â˜…ç¿»è¨³æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
                "is_prod": is_prod
            }
            
            raw_html = str(soup)
            found = set([a['href'] for a in soup.find_all('a', href=True)])
            hidden = re.findall(r'["\'](/[a-zA-Z0-9\-_/]+)[\'"]', raw_html)
            for h in hidden: found.add(h)
            
            base_d = urlparse(url).netloc
            for l in found:
                full = urljoin(url, l).rstrip('/')
                if (urlparse(full).netloc == base_d or "escortskubota" in urlparse(full).netloc):
                    if full not in visited:
                        if any(x in full.lower() for x in ['.pdf','mailto','#','javascript']): continue
                        visited.add(full)
                        cmap.setdefault(url, []).append(full)
                        np = 10
                        if any(k in full.lower() for k in ['investor','media','career','product','tractor']): np = 0
                        queue.append((np, full))
            time.sleep(0.01)
        except: continue
        
    return "ROOT_HUB", cmap, ninfo

def generate_force_html(rid, cmap, ninfo):
    total_pages = len(ninfo)
    type_counts = {}
    content_stats = {"img": 0, "spec": 0, "desc": 0}
    lang_stats = {"hi": 0, "en": 0} 
    prod_count = 0

    for url, info in ninfo.items():
        t = info.get("type", "Other")
        type_counts[t] = type_counts.get(t, 0) + 1
        if info.get("img"): content_stats["img"] += 1
        if info.get("spec"): content_stats["spec"] += 1
        if info.get("desc"): content_stats["desc"] += 1
        if "/hi/" in url or "hindi" in url: lang_stats["hi"] += 1
        else: lang_stats["en"] += 1
        if info.get("is_prod", False): prod_count += 1

    lines = ["""<!DOCTYPE html><html lang="ja"><head><meta charset="UTF-8"><title>Escorts Kubota Map with JP Summary</title>
<style>
    body { font-family: 'Segoe UI', sans-serif; background: #f0f2f5; padding: 20px; color: #333; }
    h1 { border-bottom: 3px solid #444; padding-bottom: 10px; }
    
    .tree ul { list-style: none; padding-left: 20px; }
    .tree li { margin: 0; position: relative; padding-left: 20px; padding-bottom: 10px; }
    .tree li::before {
        content: ""; position: absolute; top: 0; left: -10px;
        border-left: 2px solid #ccc; border-bottom: 2px solid #ccc;
        width: 20px; height: 20px;
    }
    .tree li::after {
        content: ""; position: absolute; top: 0; left: -10px;
        border-left: 2px solid #ccc; width: 0px; height: 100%;
    }
    .tree li:last-child::after { display: none; }
    
    .node-box { 
        display: block; background: #fff; border: 1px solid #ddd; border-left: 5px solid #999;
        border-radius: 5px; padding: 12px; box-shadow: 0 2px 5px rgba(0,0,0,0.08); width: 100%; max-width: 950px;
        position: relative; top: -5px; 
    }
    
    .lang-row { display: flex; align-items: baseline; margin-bottom: 3px; font-size: 14px; }
    .l-tag { display: inline-block; width: 30px; font-size: 10px; font-weight: bold; color: #fff; text-align: center; border-radius: 3px; margin-right: 8px; }
    .tag-ja { background-color: #333; } .tag-en { background-color: #007bff; } .tag-hi { background-color: #28a745; }
    .l-text a { text-decoration: none; color: #333; font-weight: bold; } .l-text a:hover { color: #0056b3; }
    
    .content-area { display: flex; gap: 15px; margin-top: 12px; border-top: 1px solid #eee; padding-top: 10px; flex-wrap: wrap;}
    .img-wrap { flex: 0 0 160px; text-align: center; background: #fafafa; padding: 5px; border: 1px solid #eee; border-radius: 4px; }
    .thumb-img { max-width: 100%; max-height: 120px; object-fit: contain; }
    .no-img { font-size: 11px; color: #999; padding: 20px 0; }
    
    .spec-wrap { flex: 1; min-width: 300px; font-size: 11px; max-height: 250px; overflow-y: auto; }
    
    /* è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ */
    .summary-text {
        font-size: 13px; color: #444; line-height: 1.6; background: #f0f7ff; 
        padding: 10px; border-radius: 6px; border: 1px solid #d0e1f5; margin-top: 8px;
    }
    .summary-label { font-size: 10px; font-weight: bold; color: #0056b3; text-transform: uppercase; margin-bottom: 4px; display:block; }

    .extracted-table table { width: 100%; border-collapse: collapse; }
    .extracted-table th, .extracted-table td { border: 1px solid #ddd; padding: 5px; text-align: left; }
    .extracted-table th { background: #f1f3f5; width: 35%; color: #555; }
    
    .tag-list { margin-top: 5px; }
    .c-tag { background: #e9ecef; padding: 2px 7px; border-radius: 10px; font-size: 11px; margin-right: 5px; color: #555; display: inline-block; margin-bottom: 3px;}
    
    .type-Product { border-left-color: #d63384; } .type-Product .tag-ja { background-color: #d63384; }
    .type-Farmtrac { border-left-color: #0d6efd; }
    .type-Exports { border-left-color: #17a2b8; }
    .type-Kubota { border-left-color: #28a745; } 
    .type-Construction { border-left-color: #ffc107; }
    .type-Investors { border-left-color: #20c997; background-color: #f8fffa; }
    .type-Media { border-left-color: #6f42c1; background-color: #fbf9ff; }
    .type-Careers { border-left-color: #fd7e14; background-color: #fff9f5; }
    .type-CSR { border-left-color: #6610f2; background-color: #fcfaff; }
    .type-Corporate { border-left-color: #343a40; }
    
    .type-badge { float: right; font-size: 10px; padding: 2px 8px; background: #eee; border-radius: 10px; color:#555; font-weight: bold;}

    details > summary { cursor: pointer; list-style: none; margin-bottom: 5px; font-weight: bold; color: #555; position: relative; }
    details > summary::before { content: 'ğŸ“‚'; margin-right: 5px; }
    details[open] > summary::before { content: 'ğŸ“‚'; }

    .stats-section { margin-top: 50px; padding: 30px; background: #fff; border-top: 5px solid #333; border-radius: 8px; }
    .stats-grid { display: flex; gap: 20px; margin-bottom: 30px; }
    .stat-card { flex: 1; background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border: 1px solid #eee; }
    .stat-value { font-size: 32px; font-weight: bold; color: #007bff; margin: 10px 0; }
    .stat-sub { font-size: 12px; color: #666; }
    .analysis-table { width: 100%; border-collapse: collapse; margin-bottom: 30px; }
    .analysis-table th, .analysis-table td { border: 1px solid #ddd; padding: 10px; text-align: left; }
    .analysis-table th { background: #eee; }
</style></head><body>
<h1>Escorts Kubota Map with Japanese Summary</h1>
<div class="tree"><ul>"""]
    
    seeds = set([u.rstrip('/') for u in START_URLS])

    def build(cid):
        inf = ninfo.get(cid, {})
        url = inf.get("url", "#")
        nt = inf.get("type", "Page")
        lbl = inf.get("labels", {})
        img = inf.get("img", "")
        spec = inf.get("spec", "")
        headers = inf.get("headers", [])
        desc = inf.get("desc", "")
        summary = inf.get("page_summary", "") # â˜…æ—¥æœ¬èªè¦ç´„
        
        is_corp = nt in ['Investors', 'Media', 'Careers', 'CSR', 'Corporate']
        
        lang_html = f"""
        <span class="type-badge">{nt}</span>
        <div class="lang-row"><span class="l-tag tag-ja">JA</span><div class="l-text"><a href="{url}" target="_blank">{html.escape(lbl.get('ja','-'))}</a></div></div>
        <div class="lang-row"><span class="l-tag tag-en">EN</span><div class="l-text">{html.escape(lbl.get('en','-'))}</div></div>
        <div class="lang-row"><span class="l-tag tag-hi">HI</span><div class="l-text">{html.escape(lbl.get('hi','-'))}</div></div>
        """
        
        body_html = ""
        if img or spec or desc or headers or summary:
            
            i_html = f'<div class="img-wrap"><img src="{img}" class="thumb-img"></div>' if img else ('<div class="img-wrap"><div class="no-img">No Image</div></div>' if not is_corp else '')
            
            c_html = ""
            if spec:
                c_html = f'<div class="spec-wrap">{spec}</div>'
            
            # â˜…è¦ç´„ï¼ˆæ—¥æœ¬èªï¼‰ã‚’è¡¨ç¤º
            s_html = ""
            if summary:
                s_html = f'<div class="summary-text"><span class="summary-label">æ¦‚è¦ (è‡ªå‹•ç¿»è¨³)</span>{html.escape(summary)}</div>'
            elif desc:
                # ç¿»è¨³ãŒãªã„å ´åˆã¯ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤ºï¼ˆã“ã‚Œã¯è‹±èªã®ã¾ã¾ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰
                s_html = f'<div class="summary-text"><span class="summary-label">Description</span>{html.escape(desc[:200])}</div>'
            
            if not spec and not s_html and not is_corp:
                 c_html = '<div class="spec-wrap"><div class="no-img">No Data Found</div></div>'

            t_html = '<div class="tag-list">' + "".join([f'<span class="c-tag">{html.escape(h)}</span>' for h in headers]) + '</div>'
            
            body_html = f'<div class="content-area">{i_html}<div style="flex:1;">{c_html}{s_html}{t_html}</div></div>'

        node_html = f'<div class="node-box type-{nt}">{lang_html}{body_html}</div>'
        
        ch = cmap.get(cid, [])
        if ch:
            tag = "<details open>" if cid == "ROOT_HUB" or cid in seeds else "<details>"
            lines.append(f"<li>{tag}<summary>{node_html}</summary><ul>")
            prods = [c for c in ch if ninfo.get(c,{}).get("is_prod")]
            others = [c for c in ch if c not in prods]
            for c in others + prods: build(c)
            lines.append("</ul></details></li>")
        else:
            lines.append(f"<li>{node_html}</li>")

    for c in cmap.get(rid, []): build(c)
    
    stats_html = f"""
    </ul></div>
    <div class="stats-section">
        <h2>ğŸ“Š ã‚µã‚¤ãƒˆå®šé‡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h2>
        <div class="stats-grid">
            <div class="stat-card">
                <h3>ç·å–å¾—ãƒšãƒ¼ã‚¸æ•°</h3>
                <div class="stat-value">{total_pages}</div>
                <div class="stat-sub">Page Coverage</div>
            </div>
            <div class="stat-card">
                <h3>è£½å“ãƒšãƒ¼ã‚¸æ•°</h3>
                <div class="stat-value">{prod_count}</div>
                <div class="stat-sub">{prod_count/total_pages*100:.1f}% ã®ãƒšãƒ¼ã‚¸ãŒè£½å“è©³ç´°</div>
            </div>
            <div class="stat-card">
                <h3>ç”»åƒå–å¾—ç‡</h3>
                <div class="stat-value">{content_stats['img']/total_pages*100:.1f}%</div>
                <div class="stat-sub">{content_stats['img']} ãƒšãƒ¼ã‚¸ã§ç”»åƒæ¤œå‡º</div>
            </div>
            <div class="stat-card">
                <h3>ã‚¹ãƒšãƒƒã‚¯è¡¨å–å¾—ç‡</h3>
                <div class="stat-value">{content_stats['spec']/total_pages*100:.1f}%</div>
                <div class="stat-sub">{content_stats['spec']} ãƒšãƒ¼ã‚¸ã§è©³ç´°ãƒ‡ãƒ¼ã‚¿æ¤œå‡º</div>
            </div>
        </div>
        
        <h3>ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒšãƒ¼ã‚¸æ•°</h3>
        <table class="analysis-table">
            <tr><th>ã‚«ãƒ†ã‚´ãƒª</th><th>ãƒšãƒ¼ã‚¸æ•°</th><th>æ§‹æˆæ¯”</th></tr>
            {''.join([f"<tr><td>{k}</td><td>{v}</td><td>{v/total_pages*100:.1f}%</td></tr>" for k, v in type_counts.items()])}
        </table>

        <h3>ğŸŒ è¨€èªå¯¾å¿œçŠ¶æ³</h3>
        <table class="analysis-table">
            <tr><th>è¨€èªåŒºåˆ†</th><th>æ¤œå‡ºãƒšãƒ¼ã‚¸æ•°</th><th>å‚™è€ƒ</th></tr>
            <tr><td>è‹±èª (English)</td><td>{lang_stats['en']}</td><td>æ¨™æº–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„</td></tr>
            <tr><td>ãƒ’ãƒ³ãƒ‡ã‚£ãƒ¼èª (Hindi)</td><td>{lang_stats['hi']}</td><td>ãƒ­ãƒ¼ã‚«ãƒ©ã‚¤ã‚º</td></tr>
        </table>
    </div>
    </body></html>
    """
    lines.append(stats_html)
    
    fn = r"D:\HPåˆ†æ\Escorts_With_Summary_JP.html"
    with open(fn, 'w', encoding='utf-8') as f: f.write("".join(lines))
    print(f"\nå®Œäº†ï¼ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã“ã¡ã‚‰: {fn}")

if __name__ == "__main__":
    r, c, i = crawl_force(START_URLS, MAX_PAGES)
    generate_force_html(r, c, i)
